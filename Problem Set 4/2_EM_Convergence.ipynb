{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc34b48",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 4: EM, DL & RL\n",
    "\n",
    "This is my solutions for CS229 - Fall 2017: Machine Learning taught by Andrew Ng.\n",
    "\n",
    "The material for Problem Set 4 is here: [ps4](https://github.com/nmduonggg/ML-CS229/blob/master/Problem%20Set%204/ps4.pdf)\n",
    "\n",
    "This notebook contains the solution for __Question 2: EM Convergence__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5b7d",
   "metadata": {},
   "source": [
    "### Question 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee95a2",
   "metadata": {},
   "source": [
    "The log-marginal density of the observed data is\n",
    "\n",
    "$$l(\\theta) = \\sum_{i=1}^m \\log p(x^i; \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901f1b9",
   "metadata": {},
   "source": [
    "The EM Convergence can be divided into 2 main steps:\n",
    "- E-step: \n",
    "\n",
    "$$\n",
    "Q_i(z^i) = p(z^i | x^i) = \\frac{p(x^i, z^i; \\theta)}{p(x^i)}\n",
    "$$\n",
    "\n",
    "- M-step: Maximize the following function:\n",
    "\n",
    "$$\\text{ELBO} = g(\\theta) = \\sum_i \\sum_{z^i} Q_i(z^i) \\log \\frac{p(x^i, z^i; \\theta)}{Q_i(z^i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02214d1",
   "metadata": {},
   "source": [
    "The strategy here is using the approximation function which is the lower bound of log-marginal density of the observed data to optimize $l(\\theta)$ until it converges. The convergence point of $g(\\theta)$ is actually the convergence point of $l(\\theta)$, which ensure the maximization of EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427b0a0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_\\theta g(\\theta) &= \\nabla_\\theta \\sum_i \\sum_{z^i} Q_i(z^i) \\log \\frac{p(x^i, z^i; \\theta)}{Q_i(z^i)} \\\\\n",
    "        &= \\sum_i \\sum_{z^i}Q_i(z^i) \\frac{1}{p(x^i, z^i; \\theta)} \\nabla_\\theta p(x^i, z^i; \\theta) \\\\\n",
    "        &= \\sum_i \\sum_{z^i} p(z^i|x^i) \\frac{1}{p(x^i, z^i; \\theta)} \\nabla_\\theta p(x^i, z^i; \\theta) \\\\\n",
    "        &= \\sum_i \\sum_{z^i} \\frac{p(x^i, z^i; \\theta)}{p(x^i;\\theta)p(x^i, z^i; \\theta)} \\nabla_\\theta p(x^i, z^i; \\theta) \\\\\n",
    "        &= \\sum_i \\sum_{z^i} \\frac{\\nabla_\\theta p(x^i, z^i; \\theta)}{p(x^i; \\theta)} \\\\\n",
    "        &= \\sum_i \\frac{\\nabla_\\theta p(x^i; \\theta)}{p(x^i; \\theta)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1150ae8",
   "metadata": {},
   "source": [
    "Let $\\theta^*$ the convergence point in M step, then we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta g(\\theta)_{|\\theta = \\theta^*} &= \\sum_i \\frac{\\nabla_\\theta p(x^i; \\theta)_{|\\theta = \\theta^*}}{p(x^i; \\theta^*)}\\\\\n",
    "    &= \\sum_i \\nabla_\\theta \\log \\big[p(x^i; \\theta)\\big]_{|\\theta = \\theta^*} \\\\\n",
    "    &= \\nabla_\\theta l(\\theta)_{|\\theta = \\theta^*}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701ad8e",
   "metadata": {},
   "source": [
    "This indicates that the maximizer in M step is the same as one we need to find in log marginal function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
