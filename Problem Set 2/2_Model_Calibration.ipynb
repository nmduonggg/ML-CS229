{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e31653",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 1: Supervised Learning\n",
    "\n",
    "This is my solutions for CS229 - Fall 2017: Machine Learning taught by Andrew Ng.\n",
    "\n",
    "The material for Problem Set 2 is here: [ps2](https://github.com/nmduonggg/ML-CS229/blob/master/Problem%20Set%202/ps2.pdf)\n",
    "\n",
    "This notebook contains the solution for __Question 2: Model Calibration__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a9141",
   "metadata": {},
   "source": [
    "Notation:\n",
    "- $x^i$: ith training example\n",
    "- $y^i$: ith label\n",
    "- $m$: number of training examples\n",
    "- $\\theta$: fitting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b24ea8",
   "metadata": {},
   "source": [
    "### Question 2.a)\n",
    "\n",
    "Firstly, let $\\theta \\in R^{n+1}$ be the maximum likelihood parameters learned after training the logistic model. Thus those parameters satisfies:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^m \\bigg(y^i \\log(h_\\theta(x^i)) + (1-y^i) \\log(1 - h_\\theta(x^i))\\bigg) = 0 \\\\\n",
    "\\implies \\quad & \\sum_{i=1}^m \\bigg( y^i - \\frac{1}{1 + \\exp(-\\theta^Tx^i)} \\bigg) = 0 \\\\\n",
    "\\implies \\quad & \\sum_{i=1}^m h_\\theta(x^i) = \\sum_{i=1}^m y^i \\quad (1)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811b1ee",
   "metadata": {},
   "source": [
    "In logistic regression, $\\theta$ is fit for all the given training examples with $h_\\theta(x^i) \\in (0, 1) \\ \\forall i = 1,..., m$. Then the (1) can written in the form:\n",
    "\n",
    "$$\\sum_{i \\in I_{a,b}}P(y^i = 1 | x^i; \\theta) = \\sum_{i \\in I_{a,b}} 1\\{y^i = 1\\}$$\n",
    "\n",
    "where: \n",
    "- $(a, b) = (0,1)$\n",
    "- $\\sum_{i \\in I_{a,b}} y^i = \\sum_{y=1} y^i = \\sum_{i \\in I_{a,b}} 1\\{y^i = 1\\}$\n",
    "\n",
    "Otherwise, for each range $(a, b) \\subset [0,1]$ corresponds to a different subset of originally given training set, then without constructing a new maximum likelihood function and refitting with other parameters, (1) will not hold true anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67758661",
   "metadata": {},
   "source": [
    "### Question 2.b)\n",
    "\n",
    "- When the property holds true for any $(a, b) \\subset [0,1]$, it means that the probabilities outputted by a model match emperical observations, not the prediction of that model. Thus, this does not necessarily imply that the model achieves perfect accuracy. E.g, prediction on positive class as negative and vice versa simultaneously might preserve the property withou perfect accuracy.\n",
    "\n",
    "- Conversely, model with prefectly predictive ability will be perfectly calibrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfcf44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
