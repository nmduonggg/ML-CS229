{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de1ba33",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 3: Deep Learning & Unsupervised Learning\n",
    "\n",
    "This is my solutions for CS229 - Fall 2017: Machine Learning taught by Andrew Ng.\n",
    "\n",
    "The material for Problem Set 3 is here: [ps3](https://github.com/nmduonggg/ML-CS229/blob/master/Problem%20Set%203/ps3.pdf)\n",
    "\n",
    "This notebook contains the solution for __Question 4: KL divergence and Maximum Likelihood__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc93b6c",
   "metadata": {},
   "source": [
    "### Question 4.a)\n",
    "\n",
    "Using __Jensen's inequality__ for strictly convex function $f(x) = - \\log x$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    KL(P || Q) &= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\\\\n",
    "        &= \\sum_x P(x) - \\log \\frac{Q(x)}{P(x)} \\\\\n",
    "        &= E_{x \\sim P(x)} \\bigg[ - \\log \\frac{Q(x)}{P(x)} \\bigg] \\\\\n",
    "        & \\geq -\\log E_{x \\sim P(x)} \\bigg[\\frac{Q(x)}{P(x)}\\bigg] \\\\\n",
    "        &= -\\log \\sum_x P(x) \\frac{Q(x)}{P(x)} \\\\\n",
    "        &= -\\log \\sum_x Q(x) \\\\\n",
    "        &= -\\log 1 \\\\\n",
    "        &= 0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d96d6c",
   "metadata": {},
   "source": [
    "The equality holds true if and only if $$\\frac{Q(x)}{P(x)} = E\\big[\\frac{Q(x)}{P(x)}\\big] = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9152060",
   "metadata": {},
   "source": [
    "### Question 4.b)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    KL(P(x) || Q(x)) + KL(P(y | x) || Q(y|x)) &= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} + \\sum_x P(x) \\sum_y P(y|x) \\frac{P(y|x)}{Q(y|x)} \\\\\n",
    "    &= \\sum_x \\bigg[P(x) \\log \\frac{P(x)}{Q(x)} + P(x) \\sum_y P(y|x) \\frac{P(y|x)}{Q(y|x)}\\bigg] \\\\\n",
    "    &= \\sum_x \\sum_y \\bigg[P(x, y) \\log \\frac{P(x)}{Q(x)} + P(x, y) \\log \\frac{P(y|x)}{Q(y|x)}\\bigg] \\\\\n",
    "    &= \\sum_x \\sum_y \\bigg[P(x, y) \\log \\frac{P(x) P(y|x)}{Q(x) Q(y|x)}\\bigg] \\\\\n",
    "    &= \\sum_x \\sum_y \\bigg[P(x, y) \\log \\frac{P(x, y)}{Q(x, y)}\\bigg] \\\\\n",
    "    &= KL(P(x, y) || Q(x, y))\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae7b76",
   "metadata": {},
   "source": [
    "Thus, chain rule for KL divergence holds true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7173cec1",
   "metadata": {},
   "source": [
    "### Question 4.c)\n",
    "\n",
    "Since the empirical distributions $\\hat{P}(x; \\theta)$ and $\\hat{P}(y; \\phi)$ as constant, then the minization of $KL$ divergence can be derived as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{argmin}_\\theta KL (\\hat{P}|| P_\\theta) &= KL(\\hat{P}(y) || p(y)) + \\sum_{i=1}^n KL(\\hat{P}(x_i|y) || p(x_i|y)) \\\\\n",
    "        &= \\sum_y \\hat{P}(y) \\log \\frac{\\hat{P}(y)}{p(y)} + \\sum_{i=1}^n \\sum_y \\hat{P}(y) \\sum_x \\hat{P}(x_i | y) \\log \\frac{\\hat{P}(x_i | y)}{p(x_i|y)} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This optimization proglem can be decomposed into $2n+1$ sub-problems\n",
    "\n",
    "__First problem:__\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "    &\\text{argmin}_\\phi \\sum_y \\hat{P}(y) \\log \\frac{\\hat{P}(y)}{p(y; \\phi)} = \\textit{Constant} - \\sum_y \\log p(y; \\phi) \\\\\n",
    "    \\iff \\quad &\\text{argmax}_\\phi \\sum_y \\log p(y; \\phi) \\quad \\quad \\text{(say $y$ as some fixed valued class)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27ba5e",
   "metadata": {},
   "source": [
    "__Second problem:__\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\text{argmin}_\\theta \\sum_{i=n} \\sum_{i=1}^n \\sum_y \\hat{P}(y) \\sum_x \\hat{P}(x_i | y) \\log \\frac{\\hat{P}(x_i | y)}{p(x_i|y)} \\\\\n",
    "    &= \\text{Constant} - \\sum_{i=1}^n \\sum_y \\hat{P}(y) \\sum_x \\hat{P}(x_i | y) \\log p(x_i|y)\\\\\n",
    "    &= \\text{Constant} - \\alpha \\sum_i \\sum_y \\sum_x \\log p(x_i |y)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bab66",
   "metadata": {},
   "source": [
    "Then for given $x, y$ as MLE, the above optimization problem can be written as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\text{argmin}_{\\theta, \\phi} \\quad \\log p(y; \\phi) + \\sum_{i=1}^n \\log p(x_i |y) \\\\\n",
    "\\iff \\quad &\\text{argmin}_{\\theta, \\phi} \\quad p(y) \\prod_{i=1}^n p(x_i | y) \\\\\n",
    "\\iff \\quad &\\text{argmin}_{\\theta, \\phi} \\quad P_{\\theta, \\phi}(x, y)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which is maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faef5f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
