{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad737fc",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 3: Deep Learning & Unsupervised Learning\n",
    "\n",
    "This is my solutions for CS229 - Fall 2017: Machine Learning taught by Andrew Ng.\n",
    "\n",
    "The material for Problem Set 3 is here: [ps3](https://github.com/nmduonggg/ML-CS229/blob/master/Problem%20Set%203/ps3.pdf)\n",
    "\n",
    "This notebook contains the solution for __Question 2: EM for MAP estimation__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf863a",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942300e",
   "metadata": {},
   "source": [
    "Consider the log-likelihood of MAP estimation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    l(\\theta) &= \\log \\bigg[\\prod_i \\sum_{z^i} p(x^i, z^i | \\theta) p(\\theta)\\bigg] \\\\\n",
    "        &= \\sum_i \\log \\bigg[\\sum_{z^i} p(x^i, z^i | \\theta)\\bigg] + \\log p(\\theta) \\\\\n",
    "        &= \\sum_i \\log \\sum_{z^i} Q_i(z^i) \\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)} + \\log p(\\theta) \\\\\n",
    "        & \\geq \\sum_i \\sum_{z^i} Q_i(z^i) \\log \\bigg[\\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)}\\bigg] + \\log p(\\theta)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b67d66",
   "metadata": {},
   "source": [
    "The last derivation comes from Jensen'inequality which holds true if and only if $Q_i(z^i) = p(z^i | x^i; \\theta)$. In this case, the $\\text{ELBO}(x, z; \\theta)$ should be the followings:\n",
    "\n",
    "$$\\text{ELBO}(x, z; \\theta) =  \\sum_i \\sum_{z^i} Q_i(z^i) \\log \\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)} + \\log p(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38bd32",
   "metadata": {},
   "source": [
    "Therefore, EM estimation are considered by this below process:\n",
    "\n",
    "1. E-step: Find the lower-bound for MAP in which lower-bound curve and MAP curve are really tight to each other at the current $\\theta$. Lower-bound is defined by the distribution $Q_i(z^i)$:\n",
    "\n",
    "$$Q_i(z^i) = p(z^i | x^i; \\theta) = \\frac{p(x^i, z^i; \\theta)}{\\sum_{z^i} p(x^i, z^i; \\theta)}$$\n",
    "\n",
    "2. M-step: Maximize the lower-bound curve which iteratively increases MAP function until converging to a local optimum\n",
    "\n",
    "$$\\theta = argmax_{\\theta} \\sum_i \\sum_{z^i} Q_i(z^i) \\log \\frac{p(x^i, z^i | \\theta)}{Q_i(z^i)} + \\log p(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e120864",
   "metadata": {},
   "source": [
    "__Prove MAP monotonically increases with each iteraion of EM__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbb7e6",
   "metadata": {},
   "source": [
    "Suppose we already constructed $\\text{ELBO}_t(x, z; \\theta)$ with distribution $Q_i(z^i)$ such that $l(\\theta^{(t)}) = \\text{ELBO}_t(x, z; \\theta^{(t)})$. EM then runs and tends to find the next $\\theta^{(t+1)}$ which maximizes $\\text{ELBO}_t(x, z; \\theta^{(t)})$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    l(\\theta^{(t+1)}) & \\geq \\text{ELBO}_t(x, z; \\theta^{(t+1)}) \\\\\n",
    "        & \\geq \\text{ELBO}_t(x,z; \\theta^{(t)}) = l(\\theta^{(t)})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e124d",
   "metadata": {},
   "source": [
    "For more details, readers should find in lecture notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622c8ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
